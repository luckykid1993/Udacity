{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, make_scorer, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterClean.db')\n",
    "df = pd.read_sql(\"SELECT * FROM Msg_Category_Tbl\", engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input (X) / output (Y) data\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process  text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Process text messages: Lemmatize, lower, remove blank, remove stop words\n",
    "    \n",
    "    inputs:\n",
    "        text (String): text messages\n",
    "       \n",
    "    outputs:\n",
    "        clean_tokens (list): clean tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # get list of stopwords in english\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # result\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        # Lemmatize + lower + remove blank\n",
    "        clean = lemmatizer.lemmatize(token).lower().strip()\n",
    "        # do not add stop word to result\n",
    "        if clean not in stop_words:\n",
    "            clean_tokens.append(clean)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipline with TF-IDF + RandomForestClassifier\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "pipe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test  model\n",
    "Report the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns name as list\n",
    "col_names = Y_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.96      0.89      4967\n",
      "               request       0.84      0.50      0.62      1120\n",
      "                 offer       0.00      0.00      0.00        23\n",
      "           aid_related       0.74      0.67      0.70      2679\n",
      "          medical_help       0.55      0.06      0.11       519\n",
      "      medical_products       0.82      0.10      0.18       321\n",
      "     search_and_rescue       0.75      0.02      0.04       153\n",
      "              security       0.00      0.00      0.00       124\n",
      "              military       0.67      0.07      0.13       223\n",
      "                 water       0.86      0.31      0.45       416\n",
      "                  food       0.82      0.59      0.69       724\n",
      "               shelter       0.81      0.38      0.52       579\n",
      "              clothing       0.75      0.06      0.11        97\n",
      "                 money       0.62      0.04      0.07       141\n",
      "        missing_people       0.00      0.00      0.00        82\n",
      "              refugees       0.71      0.02      0.04       241\n",
      "                 death       0.86      0.14      0.25       293\n",
      "             other_aid       0.68      0.02      0.05       865\n",
      "infrastructure_related       0.25      0.00      0.00       434\n",
      "             transport       0.67      0.05      0.10       299\n",
      "             buildings       0.76      0.12      0.20       338\n",
      "           electricity       1.00      0.02      0.05       128\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       1.00      0.02      0.03        60\n",
      "                 shops       0.00      0.00      0.00        33\n",
      "           aid_centers       0.00      0.00      0.00        76\n",
      "  other_infrastructure       0.00      0.00      0.00       299\n",
      "       weather_related       0.86      0.67      0.75      1850\n",
      "                floods       0.88      0.42      0.57       532\n",
      "                 storm       0.78      0.41      0.54       627\n",
      "                  fire       0.00      0.00      0.00        59\n",
      "            earthquake       0.90      0.77      0.83       654\n",
      "                  cold       0.80      0.09      0.16       138\n",
      "         other_weather       0.50      0.03      0.05       345\n",
      "         direct_report       0.82      0.37      0.51      1291\n",
      "\n",
      "             micro avg       0.81      0.52      0.64     20767\n",
      "             macro avg       0.59      0.20      0.25     20767\n",
      "          weighted avg       0.75      0.52      0.56     20767\n",
      "           samples avg       0.68      0.48      0.51     20767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# report the f1 score, precision and recall for each output category of the dataset using classification_report\n",
    "y_pred = pipe.predict(X_test)\n",
    "report1 = classification_report(Y_test, y_pred, target_names = col_names)\n",
    "print(report1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier()),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters of pipe\n",
    "pipe.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "              'clf__estimator__n_estimators':[50, 100, 150], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 7]}\n",
    "\n",
    "cv = GridSearchCV(estimator=pipe, param_grid=parameters, scoring='f1_weighted', verbose=3, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test  model\n",
    "Show the accuracy, precision, and recall of the tuned model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.557 total time= 3.9min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.560 total time= 3.6min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.548 total time= 3.6min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.542 total time= 6.7min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.542 total time= 6.1min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.532 total time= 6.5min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.559 total time= 7.4min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.565 total time= 6.8min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.555 total time= 6.8min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.542 total time=13.6min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.545 total time=10.1min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.535 total time= 7.2min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.560 total time= 5.9min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.567 total time= 5.5min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.552 total time= 5.6min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.545 total time=10.6min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.552 total time=10.8min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=2, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.540 total time=11.6min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.568 total time= 1.7min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.571 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.557 total time= 1.6min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.552 total time= 3.1min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.553 total time= 2.9min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.544 total time= 2.9min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.567 total time= 3.1min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.567 total time= 3.1min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.560 total time= 3.1min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.549 total time= 5.7min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.550 total time= 5.7min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.549 total time= 6.0min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.565 total time= 4.6min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.569 total time= 4.6min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.560 total time= 4.7min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.552 total time= 8.5min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.553 total time= 8.1min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=5, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.546 total time= 8.5min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.562 total time= 1.6min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.567 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.559 total time= 1.6min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.551 total time= 2.9min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.549 total time= 2.9min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.545 total time= 2.9min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.565 total time= 3.0min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.566 total time= 3.0min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 1);, score=0.557 total time= 2.9min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.552 total time= 5.5min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.553 total time= 5.0min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=100, vect__ngram_range=(1, 2);, score=0.547 total time= 5.0min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.565 total time= 4.4min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.569 total time= 4.3min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 1);, score=0.556 total time= 4.5min\n",
      "[CV 1/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.552 total time= 7.7min\n",
      "[CV 2/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.553 total time= 7.2min\n",
      "[CV 3/3] END clf__estimator__min_samples_split=7, clf__estimator__n_estimators=150, vect__ngram_range=(1, 2);, score=0.546 total time= 7.4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier()))]),\n",
       "             param_grid={'clf__estimator__min_samples_split': [2, 5, 7],\n",
       "                         'clf__estimator__n_estimators': [50, 100, 150],\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             scoring='f1_weighted', verbose=3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 5,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.96      0.89      4967\n",
      "               request       0.83      0.51      0.63      1120\n",
      "                 offer       0.00      0.00      0.00        23\n",
      "           aid_related       0.73      0.67      0.70      2679\n",
      "          medical_help       0.59      0.10      0.17       519\n",
      "      medical_products       0.82      0.14      0.24       321\n",
      "     search_and_rescue       1.00      0.02      0.04       153\n",
      "              security       0.00      0.00      0.00       124\n",
      "              military       0.67      0.07      0.13       223\n",
      "                 water       0.84      0.32      0.47       416\n",
      "                  food       0.82      0.54      0.65       724\n",
      "               shelter       0.80      0.38      0.51       579\n",
      "              clothing       0.75      0.06      0.11        97\n",
      "                 money       0.67      0.04      0.08       141\n",
      "        missing_people       1.00      0.01      0.02        82\n",
      "              refugees       0.71      0.07      0.13       241\n",
      "                 death       0.83      0.19      0.31       293\n",
      "             other_aid       0.66      0.03      0.05       865\n",
      "infrastructure_related       0.00      0.00      0.00       434\n",
      "             transport       0.72      0.09      0.16       299\n",
      "             buildings       0.79      0.10      0.18       338\n",
      "           electricity       1.00      0.01      0.02       128\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       0.00      0.00      0.00        60\n",
      "                 shops       0.00      0.00      0.00        33\n",
      "           aid_centers       0.00      0.00      0.00        76\n",
      "  other_infrastructure       0.00      0.00      0.00       299\n",
      "       weather_related       0.85      0.67      0.75      1850\n",
      "                floods       0.88      0.42      0.57       532\n",
      "                 storm       0.79      0.46      0.58       627\n",
      "                  fire       1.00      0.02      0.03        59\n",
      "            earthquake       0.91      0.77      0.83       654\n",
      "                  cold       0.75      0.09      0.16       138\n",
      "         other_weather       0.48      0.03      0.05       345\n",
      "         direct_report       0.80      0.37      0.51      1291\n",
      "\n",
      "             micro avg       0.81      0.53      0.64     20767\n",
      "             macro avg       0.61      0.20      0.26     20767\n",
      "          weighted avg       0.75      0.53      0.57     20767\n",
      "           samples avg       0.67      0.48      0.51     20767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# predict and show report\n",
    "y_pred2 = cv.predict(X_test)\n",
    "report2 = classification_report(Y_test, y_pred2, target_names = col_names)\n",
    "print(report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving  model further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use other argothrims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier\n",
    "pipe_ada = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDClassifier\n",
    "pipe_sgdc = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will try to use other methods to improve my model\n",
    "# so i create this fuction to reuse many times\n",
    "# train and create test report\n",
    "def train_test_model(pipe, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"train model and create test report using classification_report\n",
    "    \n",
    "    inputs:\n",
    "        pipe: model\n",
    "        x_train, y_train: train data\n",
    "        x_test, y_test: test data\n",
    "       \n",
    "    outputs:\n",
    "        report: report generated by classification_report\n",
    "    \"\"\"\n",
    "    # train\n",
    "    pipe.fit(x_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    \n",
    "    # columns name\n",
    "    col_names_list = y_train.columns.tolist()\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names = col_names_list)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.92      0.87      4967\n",
      "               request       0.76      0.52      0.62      1120\n",
      "                 offer       0.00      0.00      0.00        23\n",
      "           aid_related       0.74      0.59      0.66      2679\n",
      "          medical_help       0.58      0.23      0.33       519\n",
      "      medical_products       0.62      0.33      0.43       321\n",
      "     search_and_rescue       0.45      0.13      0.20       153\n",
      "              security       0.19      0.02      0.04       124\n",
      "              military       0.60      0.33      0.43       223\n",
      "                 water       0.76      0.66      0.71       416\n",
      "                  food       0.80      0.67      0.73       724\n",
      "               shelter       0.75      0.54      0.63       579\n",
      "              clothing       0.66      0.38      0.48        97\n",
      "                 money       0.45      0.26      0.33       141\n",
      "        missing_people       0.48      0.13      0.21        82\n",
      "              refugees       0.51      0.18      0.26       241\n",
      "                 death       0.75      0.43      0.55       293\n",
      "             other_aid       0.49      0.12      0.20       865\n",
      "infrastructure_related       0.42      0.09      0.14       434\n",
      "             transport       0.62      0.25      0.36       299\n",
      "             buildings       0.67      0.38      0.49       338\n",
      "           electricity       0.55      0.31      0.40       128\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       0.24      0.07      0.10        60\n",
      "                 shops       0.00      0.00      0.00        33\n",
      "           aid_centers       0.22      0.08      0.12        76\n",
      "  other_infrastructure       0.39      0.08      0.14       299\n",
      "       weather_related       0.85      0.65      0.74      1850\n",
      "                floods       0.83      0.52      0.64       532\n",
      "                 storm       0.78      0.50      0.61       627\n",
      "                  fire       0.61      0.19      0.29        59\n",
      "            earthquake       0.89      0.76      0.82       654\n",
      "                  cold       0.68      0.28      0.40       138\n",
      "         other_weather       0.52      0.16      0.24       345\n",
      "         direct_report       0.72      0.45      0.56      1291\n",
      "\n",
      "             micro avg       0.77      0.57      0.65     20767\n",
      "             macro avg       0.55      0.32      0.39     20767\n",
      "          weighted avg       0.73      0.57      0.62     20767\n",
      "           samples avg       0.62      0.49      0.50     20767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# train + test AdaBoostClassifier\n",
    "report_ada = train_test_model(pipe_ada, X_train, Y_train, X_test, Y_test)\n",
    "print(report_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      1.00      0.87      4967\n",
      "               request       0.87      0.35      0.50      1120\n",
      "                 offer       0.00      0.00      0.00        23\n",
      "           aid_related       0.81      0.43      0.56      2679\n",
      "          medical_help       0.00      0.00      0.00       519\n",
      "      medical_products       0.00      0.00      0.00       321\n",
      "     search_and_rescue       0.00      0.00      0.00       153\n",
      "              security       0.00      0.00      0.00       124\n",
      "              military       0.00      0.00      0.00       223\n",
      "                 water       0.86      0.09      0.17       416\n",
      "                  food       0.80      0.45      0.58       724\n",
      "               shelter       0.88      0.19      0.32       579\n",
      "              clothing       0.00      0.00      0.00        97\n",
      "                 money       0.00      0.00      0.00       141\n",
      "        missing_people       0.00      0.00      0.00        82\n",
      "              refugees       0.00      0.00      0.00       241\n",
      "                 death       0.00      0.00      0.00       293\n",
      "             other_aid       0.00      0.00      0.00       865\n",
      "infrastructure_related       0.00      0.00      0.00       434\n",
      "             transport       0.00      0.00      0.00       299\n",
      "             buildings       0.00      0.00      0.00       338\n",
      "           electricity       0.00      0.00      0.00       128\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       0.00      0.00      0.00        60\n",
      "                 shops       0.00      0.00      0.00        33\n",
      "           aid_centers       0.00      0.00      0.00        76\n",
      "  other_infrastructure       0.00      0.00      0.00       299\n",
      "       weather_related       0.88      0.38      0.53      1850\n",
      "                floods       0.94      0.06      0.11       532\n",
      "                 storm       0.64      0.04      0.07       627\n",
      "                  fire       0.00      0.00      0.00        59\n",
      "            earthquake       0.92      0.49      0.64       654\n",
      "                  cold       0.00      0.00      0.00       138\n",
      "         other_weather       0.00      0.00      0.00       345\n",
      "         direct_report       0.84      0.21      0.34      1291\n",
      "\n",
      "             micro avg       0.80      0.40      0.53     20767\n",
      "             macro avg       0.26      0.11      0.13     20767\n",
      "          weighted avg       0.61      0.40      0.43     20767\n",
      "           samples avg       0.73      0.42      0.48     20767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# train + test SGDClassifier\n",
    "report_sgdc = train_test_model(pipe_sgdc, X_train, Y_train, X_test, Y_test)\n",
    "print(report_sgdc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoostClassifier appears to be better suited to my model.\n",
    "So i will stick with AdaBoostClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to tunning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultiOutputClassifier(estimator=AdaBoostClassifier()))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(tokenizer=<function tokenize at 0x0000019ABEC091F0>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier()),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters of pipe\n",
    "pipe_ada.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_ada = {'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "                  'clf__estimator__learning_rate': [0.5, 1.0, 1.5, 2.0],\n",
    "                  'clf__estimator__n_estimators':[25, 50, 75]}\n",
    "\n",
    "cv_ada = GridSearchCV(estimator=pipe_ada, param_grid=parameters_ada, scoring='f1_weighted', verbose=3, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.552 total time=  38.5s\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.562 total time=  38.1s\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.552 total time=  40.6s\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.559 total time= 1.3min\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.563 total time= 1.2min\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.558 total time= 1.3min\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.585 total time= 1.1min\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.590 total time= 1.1min\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.587 total time= 1.1min\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.593 total time= 2.4min\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.596 total time= 2.3min\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.592 total time= 2.2min\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.601 total time= 1.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.604 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.600 total time= 1.7min\n",
      "[CV 1/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.607 total time= 3.8min\n",
      "[CV 2/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.609 total time= 3.8min\n",
      "[CV 3/3] END clf__estimator__learning_rate=0.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.605 total time= 3.8min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.609 total time=  40.0s\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.613 total time=  38.9s\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.607 total time=  39.2s\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.613 total time= 1.3min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.621 total time= 1.3min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.608 total time= 1.4min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.625 total time= 1.1min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.620 total time= 1.1min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.624 total time= 1.1min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.626 total time= 2.5min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.624 total time= 2.5min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.627 total time= 2.5min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.627 total time= 1.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.629 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.626 total time= 1.6min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.631 total time= 3.8min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.629 total time= 4.0min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.630 total time= 3.9min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.609 total time=  40.1s\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.601 total time=  39.5s\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.592 total time=  40.5s\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.598 total time= 1.4min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.593 total time= 1.4min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.584 total time= 1.4min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.616 total time= 1.2min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.617 total time= 1.1min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.617 total time= 1.2min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.612 total time= 2.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.616 total time= 2.5min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.622 total time= 2.6min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.617 total time= 1.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.616 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.612 total time= 1.6min\n",
      "[CV 1/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.614 total time= 3.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.618 total time= 3.6min\n",
      "[CV 3/3] END clf__estimator__learning_rate=1.5, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.608 total time= 3.7min\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.457 total time=  39.8s\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.458 total time=  39.1s\n",
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 1);, score=0.459 total time=  38.7s\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.461 total time= 1.3min\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.462 total time= 1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=25, vect__ngram_range=(1, 2);, score=0.461 total time= 1.3min\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.209 total time= 1.1min\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.211 total time= 1.1min\n",
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 1);, score=0.208 total time= 1.1min\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.201 total time= 2.4min\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.207 total time= 2.5min\n",
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=50, vect__ngram_range=(1, 2);, score=0.203 total time= 2.5min\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.462 total time= 1.6min\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.462 total time= 1.6min\n",
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 1);, score=0.458 total time= 1.6min\n",
      "[CV 1/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.468 total time= 3.2min\n",
      "[CV 2/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.467 total time= 3.2min\n",
      "[CV 3/3] END clf__estimator__learning_rate=2.0, clf__estimator__n_estimators=75, vect__ngram_range=(1, 2);, score=0.471 total time= 3.2min\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.87      0.84      0.86      4967\n",
      "               request       0.70      0.57      0.63      1120\n",
      "                 offer       0.05      0.04      0.05        23\n",
      "           aid_related       0.74      0.62      0.67      2679\n",
      "          medical_help       0.58      0.28      0.38       519\n",
      "      medical_products       0.61      0.38      0.47       321\n",
      "     search_and_rescue       0.35      0.16      0.22       153\n",
      "              security       0.27      0.10      0.14       124\n",
      "              military       0.58      0.35      0.43       223\n",
      "                 water       0.70      0.72      0.71       416\n",
      "                  food       0.78      0.68      0.73       724\n",
      "               shelter       0.72      0.56      0.63       579\n",
      "              clothing       0.59      0.48      0.53        97\n",
      "                 money       0.49      0.31      0.38       141\n",
      "        missing_people       0.27      0.09      0.13        82\n",
      "              refugees       0.52      0.19      0.27       241\n",
      "                 death       0.72      0.45      0.56       293\n",
      "             other_aid       0.46      0.17      0.24       865\n",
      "infrastructure_related       0.43      0.10      0.16       434\n",
      "             transport       0.56      0.18      0.27       299\n",
      "             buildings       0.63      0.41      0.50       338\n",
      "           electricity       0.56      0.30      0.39       128\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       0.18      0.12      0.14        60\n",
      "                 shops       0.14      0.06      0.09        33\n",
      "           aid_centers       0.16      0.12      0.14        76\n",
      "  other_infrastructure       0.30      0.08      0.13       299\n",
      "       weather_related       0.84      0.68      0.75      1850\n",
      "                floods       0.76      0.54      0.63       532\n",
      "                 storm       0.74      0.60      0.66       627\n",
      "                  fire       0.45      0.17      0.25        59\n",
      "            earthquake       0.87      0.76      0.81       654\n",
      "                  cold       0.61      0.39      0.48       138\n",
      "         other_weather       0.48      0.19      0.27       345\n",
      "         direct_report       0.67      0.48      0.56      1291\n",
      "\n",
      "             micro avg       0.75      0.57      0.65     20767\n",
      "             macro avg       0.53      0.35      0.41     20767\n",
      "          weighted avg       0.72      0.57      0.63     20767\n",
      "           samples avg       0.56      0.46      0.46     20767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lucky\\anaconda3\\envs\\env_udacity\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_cv_ada = train_test_model(cv_ada, X_train, Y_train, X_test, Y_test)\n",
    "print(report_cv_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 75,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ada.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export  model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_ada, open('classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
